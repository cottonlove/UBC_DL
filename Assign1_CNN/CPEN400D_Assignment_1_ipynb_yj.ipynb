{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9AS6zKmYSwVD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Programming Assignment 1: Convolution and Back-Propagation\n",
        "\n",
        "**UBC CPEN 400D: Deep Learning, 2022 Winter Term 2**\n",
        "\n",
        "**Created By Renjie Liao**\n",
        "\n",
        "**Date: Jan. 24, 2023**"
      ],
      "metadata": {
        "id": "9AS6zKmYSwVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Setup\n",
        "\n",
        "We will use PyTorch to implement this assignment."
      ],
      "metadata": {
        "id": "HhakGhuGTBzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PoqjO9gIKMMV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pdb\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from math import pi\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "## load MNIST images\n",
        "B = 5 # batch size\n",
        "train_set = datasets.MNIST('./data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())\n",
        "\n",
        "loader = torch.utils.data.DataLoader(train_set, batch_size=B)\n",
        "\n",
        "## load a batch of MNIST images as a PyTorch tensor (shape: B x C x H x W)\n",
        "# B: batch size\n",
        "# C: number of channels\n",
        "# H: height of images\n",
        "# W: width of images\n",
        "img, label = next(iter(loader)) # img shape: B x C x H x W, label shape: B X 1\n",
        "\n",
        "## create a random filter (shape: D x C x K x K)\n",
        "K = 3 # kernel size\n",
        "P = 1 # padding size\n",
        "C = 1 # channel size\n",
        "D = 2 # number of filters\n",
        "filter = torch.randn(D, C, K, K) # filter shape: D x C x K x K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad_img_1_1 = np.pad(img_1_1, ((p,p),(p,p)), 'constant', constant_values = 0)\n",
        "def padding_scratch(img, padding):\n",
        "  h, w = img.shape\n",
        "  h_pad, w_pad = h+2*padding, w+2*padding\n",
        "  padded_img = torch.zeros((h_pad, w_pad))\n",
        "  for i in range(h):\n",
        "    for j in range(w):\n",
        "      padded_img[padding+i][padding+j] = img[i][j]\n",
        "  return padded_img\n"
      ],
      "metadata": {
        "id": "CAUuQRRJuBzN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padded_img = np.pad(img, ((0,0), (0,0),(pad,pad), (pad,pad)), 'constant', constant_values = (0,0))\n",
        "def padding_scratch4(img, pad):\n",
        "  b,c,h,w = img.shape\n",
        "  h_pad, w_pad = h+2*pad, w+2*P\n",
        "  # print(b,c,h,w)\n",
        "  padded_img = torch.zeros((b,c,h_pad, w_pad))\n",
        "  for i in range(b):\n",
        "    for j in range(c):\n",
        "      for k in range(h):\n",
        "        for l in range(w):\n",
        "          padded_img[i][j][pad+k][pad+l] = img[i][j][k][l]\n",
        "  return padded_img\n"
      ],
      "metadata": {
        "id": "qcar_gnXwl5Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.pad(h, ((hp),(hp)), 'constant', constant_values = 0) #need it for cyclic_matrix column #(2wh - \n",
        "def padding_scratch_vec(vec, pad):\n",
        "  print(vec.shape)\n",
        "  h = vec.shape[0]\n",
        "  h_pad = h+2*pad\n",
        "  padded_vector = torch.zeros((h_pad,))\n",
        "  for i in range(h):\n",
        "    padded_vector[pad+i] = vec[i]\n",
        "  return padded_vector\n"
      ],
      "metadata": {
        "id": "qoK2WqgUGMUq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = np.ones((2,3))\n",
        "# padded_img = padding_scratch(img, 2)\n",
        "# print(padded_img)"
      ],
      "metadata": {
        "id": "9uaX0YSjvGQB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #remove before submit it.\n",
        "# print(\"the img shape is \", img.shape)\n",
        "# print(\"filter shape is\", filter.shape)"
      ],
      "metadata": {
        "id": "K4mxOMbixF2S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_0 = img[0]\n",
        "# print(img_0.shape)\n",
        "# img_1 = img[1]\n",
        "# print(img_1.shape)\n",
        "# img_2 = img[2]\n",
        "# print(img_2.shape)\n",
        "# img_1_1 = img_1[0]\n",
        "# print(img_1_1.shape)\n",
        "# img_1_1_1 = img_1_1[0]\n",
        "# print(img_1_1_1.shape)"
      ],
      "metadata": {
        "id": "n3ue46sc_BnN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter_1 = filter[1]\n",
        "# print(filter_1.shape)\n",
        "# filter_1_1 = filter_1[0]\n",
        "# print(filter_1_1.shape)\n",
        "# print(filter_1_1)\n",
        "# vectorized_1d_filter = filter_1_1.flatten() #1x(KxK)\n",
        "# print(vectorized_1d_filter.shape)\n",
        "# print(vectorized_1d_filter)"
      ],
      "metadata": {
        "id": "uMSvuJ6__hq7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(img)"
      ],
      "metadata": {
        "id": "xEQaqYgQ6SnT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B, C, H, W = img.shape\n",
        "# print(B)\n",
        "# print(C)\n",
        "# print(H)\n",
        "# print(W)"
      ],
      "metadata": {
        "id": "yi07qK8qyknV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_w = (W+2*1 - K)/1 + 1\n",
        "# output_h = (H+2*1 - K)/1 + 1\n",
        "\n",
        "# print(output_w, output_h)"
      ],
      "metadata": {
        "id": "9Fd_NgDtBUpQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k = 3\n",
        "# s = 1\n",
        "# for i in range(int(output_w)):\n",
        "#   for j in range(int(output_h)):\n",
        "#     print(0+s*i, k+s*i)\n",
        "#     print(0+s*j, k+s*j)"
      ],
      "metadata": {
        "id": "75EWDvwufn2t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum_matrix = np.zeros((4,4))\n",
        "# result_matrix = np.ones((4,4))\n",
        "# print(sum_matrix)\n",
        "# print(result_matrix)\n",
        "# final_matrix = sum_matrix + result_matrix\n",
        "# print(final_matrix)\n",
        "\n",
        "# #slicing\n",
        "# mid_matrix = final_matrix[0:2, 0:2]\n",
        "# print(mid_matrix.shape)\n",
        "# print(mid_matrix)"
      ],
      "metadata": {
        "id": "0egi2k3tCAVn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p = 1"
      ],
      "metadata": {
        "id": "Xf5Yis__IAsh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad_img_1_1 = np.pad(final_matrix, ((p,p),(p,p)), 'constant', constant_values = 0)\n",
        "# print(pad_img_1_1.shape)\n",
        "# print(pad_img_1_1)"
      ],
      "metadata": {
        "id": "bP4AWPeWH-L6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Q1 [60Pts]: 2D convolution and its gradient\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6t6YO5QlYhbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 [30Pts]  Implement 2D convolution:\n",
        "\n",
        "Discrete convolution can be implemented in multiple ways, e.g., matrix multiplication in spatial/Fourier domains.\n",
        "\n",
        "First, let us take a look at 1D convolution in spatial domain. Suppose we have a 1D signal with $n$ elements $x_1, x_2, \\dots, x_n$ and a 1D filter with $m$ weights $h_1, h_2, \\dots, h_m$. Note that we typically use filters with odd sizes for the ease of indexing.\n",
        "\n",
        "The (discrete) convolution with zero-padding and stride 1 is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "    y = h \\ast x = \\sum_{i=1}^{n} \\sum_{j=1}^{m} h_j x_{i - \\lfloor m/2 \\rfloor - 1 + j},\n",
        "\\end{align}\n",
        "where padded values $x_{-\\lfloor m/2 \\rfloor + 1}, \\dots, x_{0}, x_{n+1}, \\dots, x_{n - \\lfloor m/2 \\rfloor - 1 + m}$ are all zeros.\n",
        "\n",
        "If you forget about the concepts of padding and stride, take a look at [this guide](https://arxiv.org/pdf/1603.07285.pdf) or [these pictures](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
        "\n",
        "In the 1D case, we can illustrate the two matrix multiplication views of spatial convolution as below.\n",
        "\n",
        "1.   **im2col**: \n",
        "The key idea is to first **extract the spatial windows from the signal** $x$ for individual convolutions and then perform convolutions (i.e., dot product with the filter).\n",
        "If we put each window as a column in a matrix (the right one in RHS below), then we can perform convolution via the following matrix multiplication (N.B.: the products between the filter and individual columns can be done in parallel).\n",
        "\n",
        "\\begin{align}\n",
        "    y^\\top = (h \\ast x)^{\\top} = \\begin{bmatrix}\n",
        "                h_m & h_{m-1} & \\cdots & h_3 & h_2 & h_1\n",
        "            \\end{bmatrix}\n",
        "            \\begin{bmatrix}\n",
        "                x_{m - \\lfloor m/2 \\rfloor} & x_{m - \\lfloor m/2 \\rfloor + 1} & \\cdots & x_m & x_{m+1} & \\cdots & 0 & 0 \\\\\n",
        "                \\vdots & \\vdots & \\cdots & x_{m-1} & x_m & \\cdots & \\vdots & \\vdots \\\\\n",
        "                x_1 & x_2 & \\cdots & \\vdots & x_{m-1} & \\cdots  & x_n & 0 \\\\\n",
        "                0 & x_1 & \\cdots & \\vdots & \\vdots & \\cdots  & x_{n-1} & x_n \\\\\n",
        "                \\vdots & 0 & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots \\\\                        \n",
        "                0 & 0 & \\cdots & x_1 & x_2 & \\cdots & x_{n - \\lfloor m/2 \\rfloor+1} & x_{n - \\lfloor m/2 \\rfloor}\n",
        "            \\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "2.   **filter2row**: The key idea is to convert the filter and the signal to a sparse cyclic matrix and a vector respectively.\n",
        "Then the convolution is simply the matrix multiplication between the filter and the signal.\n",
        "\n",
        "\\begin{align}\n",
        "        y = h \\ast x = \n",
        "            \\begin{bmatrix}\n",
        "                h_{\\lfloor m/2 \\rfloor + 1} & h_{\\lfloor m/2 \\rfloor + 2} & \\cdots & h_m & 0 & \\cdots & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
        "                h_{\\lfloor m/2 \\rfloor} & h_{\\lfloor m/2 \\rfloor + 1} & \\cdots & h_{m-1} & h_m & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
        "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "                h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & 0 & \\cdots & 0 \\\\\n",
        "                0 & h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & \\cdots & 0 \\\\\n",
        "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "                0 & 0 & \\cdots & \\cdots & \\cdots & 0 & h_1 & h_2 & \\cdots & h_{\\lfloor m/2 \\rfloor + 2} \\\\                \n",
        "                0 & 0 & \\cdots & \\cdots & & \\cdots \\cdots & 0 & h_1 & \\cdots & h_{\\lfloor m/2 \\rfloor + 1}\n",
        "            \\end{bmatrix}\n",
        "            \\begin{bmatrix}\n",
        "                x_1 \\\\\n",
        "                x_2 \\\\\n",
        "                x_3 \\\\\n",
        "                \\vdots \\\\\n",
        "                x_n\n",
        "            \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Task**: \n",
        "Implement the 2D convolution in the spatial domain via matrix multiplication following the above two views: **im2col** and **filter2row**.\n",
        "The starter code is provided below.\n",
        "You just need to fill in the missing parts of function ***conv2d_im2col*** and ***conv2d_filter2row***.\n",
        "If your implementation is correct, the ***unit_test*** will output:\n",
        "\n",
        "*Your implementation of xxx is correct!*\n",
        "\n",
        "Otherwise, it will output:\n",
        "\n",
        "*Your implementation of xxx is wrong!*\n",
        "\n",
        "**N.B.**: we assume the strides along height and width are the same and the kernel is square\n",
        "\n",
        "**Hint**: you can reduce the 2D case to 1D and follow the above construction."
      ],
      "metadata": {
        "id": "koMGWjFvWWOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector = np.zeros((9,))\n",
        "# print(vector)\n",
        "# padd = 2\n",
        "# padd_vector = np.pad(vector, ((padd),(padd)), 'constant', constant_values = 1)\n",
        "# print(padd_vector.shape)\n",
        "# print(padd_vector)"
      ],
      "metadata": {
        "id": "SZdGN8WYsgLG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padd_vector[0] = 2\n",
        "# print(padd_vector)\n",
        "# # print(padd_vector.reverse()) #'numpy.ndarray' object has no attribute 'reverse'\n",
        "# #brr[:] = brr[::-1]\n",
        "# reverse = padd_vector[::-1]\n",
        "# print(reverse)"
      ],
      "metadata": {
        "id": "T3t8Sz-6y12Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix = np.zeros((3,3))\n",
        "# print(matrix)\n",
        "# # matrix[0:1, 1] = 1 #(0,1)\n",
        "# # print(matrix)\n",
        "# for i in range(3):\n",
        "#   matrix[i:i+1, 1] = 1 #column[1] = [1,1,1] \n",
        "# print(matrix)\n",
        "\n",
        "# print(\"hi\")\n",
        "# print(matrix[0, :])\n",
        "# print(matrix[0, :].shape)\n",
        "\n",
        "# print(vector[0:3].shape)\n",
        "# print(vector[0:3])\n",
        "\n",
        "# # cyclic_matrix[jeno:jeno+1,yj] = padded_h[0+yj:yj + (k*k)//2]"
      ],
      "metadata": {
        "id": "-9u7BXRquzZM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix = np.zeros((3,3))\n",
        "# print(matrix)\n",
        "# vector = [1,2,3]\n",
        "# matrix[:, 0] = vector\n",
        "# print(matrix)\n",
        "# # reverse = slide[::-1]\n",
        "# reverse = vector[::-1]\n",
        "# matrix[:, 1] = reverse\n",
        "# print(matrix)"
      ],
      "metadata": {
        "id": "FYQSg5Y76WrM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following two functions\n",
        "def conv2d_im2col(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "\n",
        "    #get input shape\n",
        "    B, C, H, W = img.shape\n",
        "    #get filter shape\n",
        "    D, C, K, K = filter.shape\n",
        "    D = num_filters ##\n",
        "    C = channel_size ##\n",
        "    K = kernel_size ##\n",
        "\n",
        "    #get stride, padding\n",
        "    s = stride\n",
        "    p = padding\n",
        "\n",
        "    #shape of output w', h'\n",
        "    output_w = (W+2*p - K)/s + 1\n",
        "    output_h = (H+2*p - K)/s + 1\n",
        "\n",
        "    #(D, h', w')\n",
        "    d3_tensor = torch.zeros((D, int(output_h), int(output_w)))\n",
        "\n",
        "    #(B, D, h', w')\n",
        "    d4_tensor = torch.zeros((B, D, int(output_h), int(output_w)))\n",
        "\n",
        "    # #do padding first\n",
        "    # np.pad(bb, ((2,2),(2,2)), 'constant', constant_values=0)\n",
        "    for i in range(B): #batch imgs -> 1 img\n",
        "      img_1 = img[i] #img_1 shape:  C x H x W\n",
        "      \n",
        "      for j in range(D): #D filters -> 1 filter\n",
        "        filter_1 = filter[j] #filter_1 shape: C x K x K\n",
        "        \n",
        "        for k in range(C): #C channels -> 1 channel (2d convolution 2d)\n",
        "          img_1_1 = img_1[k] #img_1_1 shape: H x W\n",
        "          filter_1_1 = filter_1[k] #filter_1_1 shape: K x K\n",
        "          #padding\n",
        "          # pad_img_1_1 = np.pad(img_1_1, ((p,p),(p,p)), 'constant', constant_values = 0)\n",
        "          pad_img_1_1 = padding_scratch(img_1_1, p)\n",
        "          sum_matrix = torch.zeros((int(output_h), int(output_w)))\n",
        "          result_matrix = torch.zeros((int(output_h), int(output_w)))\n",
        "\n",
        "          \"\"\"\n",
        "          result_matrix = ~~~\n",
        "          sum_matrix += result_matrix (for loop로 element-wise addition해줘야 하나?)\n",
        "          \"\"\"\n",
        "          vectorized_1d_filter = filter_1_1.flatten() #1x(KxK)\n",
        "          img2_col = torch.zeros((K*K, int(output_w*output_h))) #(kxk)x(w'xh')\n",
        "\n",
        "          for yj in range(int(output_h)):\n",
        "            for jeno in range(int(output_w)):\n",
        "              window = pad_img_1_1[0+s*yj:K+s*yj, 0+s*jeno:K+s*jeno]\n",
        "              #print(\"window shape is \", window.shape)\n",
        "              flat_window = window.flatten()\n",
        "              #print(\"flat window shape is \", flat_window.shape)  #column vector (KxK)x1\n",
        "              \n",
        "              # column_window = np.transpose(flat_window) #column vector (KxK)x1\n",
        "              # print(\"column window shape is\", column_window.shape)\n",
        "              img2_col[:, int(yj*output_h + jeno)] = flat_window #0~output_w*output_h-1\n",
        "          \n",
        "          # for yj in range(output_w*output_h): #slide = output_w*output_h 개\n",
        "          #     print(\"hello\")\n",
        "          #     window = pad_img_1_1[0:k,0:k] #slicing. window shape: kxk #need to be changed\n",
        "          #     flat_window = window.flatten()\n",
        "          #     column_window = np.transpose(flat_window) #column vector. (KxK)x1\n",
        "          #     img2_col[:, yj] = column_window\n",
        "          \n",
        "          #print(vectorized_1d_filter.shape)\n",
        "          #print(img2_col.shape)\n",
        "          result_matrix = torch.matmul(vectorized_1d_filter, img2_col)\n",
        "          #result_matrix = torch.dot(vectorized_1d_filter, img2_col)\n",
        "          #print(result_matrix.shape) #(output_w)x(output_h)\n",
        "          result_matrix = torch.reshape(result_matrix, (int(output_w), int(output_h)))\n",
        "          sum_matrix = sum_matrix + result_matrix #sum_matrix: (output_w)x(output_h)\n",
        "\n",
        "          # print(\"hi\")\n",
        "          # print(\"sum_matrix.shape is \", sum_matrix.shape)\n",
        "      #다시 up dimension해줘야.\n",
        "        d3_tensor[j] = sum_matrix\n",
        "   #다시 up dimension해줘야. \n",
        "      d4_tensor[i] = d3_tensor\n",
        "    return d4_tensor #torch.tensor(d4_tensor)\n",
        "    #return output # convoluted images, shape B x D x H x W\n",
        "    # pass\n",
        "\n",
        "\n",
        "def conv2d_filter2row(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #get input shape\n",
        "    B, C, H, W = img.shape\n",
        "    #get filter shape\n",
        "    D, C, K, K = filter.shape\n",
        "    D = num_filters ##\n",
        "    C = channel_size ##\n",
        "    K = kernel_size ##\n",
        "\n",
        "    #get stride, padding\n",
        "    s = stride\n",
        "    p = padding\n",
        "\n",
        "    #shape of output w', h'\n",
        "    output_w = (W+2*p - K)/s + 1\n",
        "    output_h = (H+2*p - K)/s + 1\n",
        "\n",
        "    #(D, h', w')\n",
        "    d3_tensor = torch.zeros((D, int(output_w), int(output_h)))\n",
        "\n",
        "    #(B, D, h', w')\n",
        "    d4_tensor = torch.zeros((B, D, int(output_w), int(output_h)))\n",
        "\n",
        "    # #do padding first\n",
        "    # np.pad(bb, ((2,2),(2,2)), 'constant', constant_values=0)\n",
        "  #   for i in range(B): #batch imgs -> 1 img\n",
        "  #     img_1 = img[i] #img_1 shape:  C x H x W\n",
        "      \n",
        "  #     for j in range(D): #D filters -> 1 filter\n",
        "  #       filter_1 = filter[j] #filter_1 shape: C x K x K\n",
        "        \n",
        "  #       for k in range(C): #C channels -> 1 channel (2d convolution 2d)\n",
        "  #         img_1_1 = img_1[k] #img_1_1 shape: H x W\n",
        "  #         filter_1_1 = filter_1[k] #filter_1_1 shape: K x K\n",
        "  #         #padding\n",
        "  #         #pad_img_1_1 = np.pad(img_1_1, ((p,p),(p,p)), 'constant', constant_values = 0)\n",
        "  #         #print(\"pad_img_1_1 shape is \", pad_img_1_1.shape) #30 x 30\n",
        "  #         sum_matrix = torch.zeros((int(output_w), int(output_h)))\n",
        "  #         result_matrix = torch.zeros((int(output_w), int(output_h)))\n",
        "\n",
        "  #         cyclic_matrix = torch.zeros((int(output_w*output_h), int(output_w*output_h))) #hw * hw\n",
        "  #         vectorized_img = img_1_1.flatten() #hw*1 = 784\n",
        "  #         #print(vectorized_img.shape)\n",
        "  #         h = filter_1_1.flatten() #(KxK)x1. h = [h1, h2, ... h[k^2/2]+1, hk^2]\n",
        "  #         #print(\"h shape is \",h.shape)\n",
        "  #         hp = int(output_w*output_h - (K*K//2) -1) #of padding\n",
        "  #         #np.pad\n",
        "  #         padded_h = padding_scratch_vec(h, hp) #np.pad(h, ((hp),(hp)), 'constant', constant_values = 0) #need it for cyclic_matrix column #(2wh - 1)x1\n",
        "  #         #print(\"padded_h shape is \", padded_h.shape) #2*w*h - 1. (1567, )\n",
        "          \n",
        "  #         #for loop. make cyclic_matrix\n",
        "  #         for yj in range(int(output_w*output_h)): #yj: column index. 0~(wh-1)\n",
        "  #           # for jeno in range(int(output_w*output_h)): #jeno: row index. 0~(wh-1)\n",
        "  #           # print(cyclic_matrix[:,yj].shape)\n",
        "  #           # print(padded_h[(0+yj):(yj + int(output_h*output_w))].shape)\n",
        "  #           slide = padded_h[(0+yj):(yj + int(output_h*output_w))]\n",
        "  #           #print(\"slide shape is\", slide.shape)\n",
        "            \n",
        "  #           # print(\"0+yj is \", 0+yj)\n",
        "  #           # print(\"yj + int(output_h*output_w is \", yj + int(output_h*output_w))\n",
        "  #           #brr[:] = brr[::-1]\n",
        "\n",
        "  #           reverse = slide[::-1]\n",
        "            \n",
        "  #           cyclic_matrix[:,yj] = reverse\n",
        "  #           # print(matrix[0, :].shape)\n",
        "  #           # print(vector[0:3].shape)\n",
        "\n",
        "  #         # print(\"cyclic_matrix[:, 0]\", cyclic_matrix[:, 0])\n",
        "  #         # print(\"cyclic_matrix[:, -1]\", cyclic_matrix[:, -1])\n",
        "  #         break\n",
        "  #         result_matrix = torch.matmul(cyclic_matrix, vectorized_img)\n",
        "  #         # print(\"result_matrix.shape is \", result_matrix.shape) #hw x 1\n",
        "\n",
        "  #         #print(result_matrix.shape) #(output_w)x(output_h)\n",
        "  #         result_matrix = torch.reshape(result_matrix, (int(output_w), int(output_h)))\n",
        "  #         sum_matrix = sum_matrix + result_matrix #sum_matrix: (output_w)x(output_h)\n",
        "          \n",
        "  #         #print(\"hi\")\n",
        "  #         #print(\"sum_matrix.shape is \", sum_matrix.shape)\n",
        "  #     #다시 up dimension해줘야.\n",
        "  #       d3_tensor[j] = sum_matrix\n",
        "  #  #다시 up dimension해줘야. \n",
        "  #     d4_tensor[i] = d3_tensor\n",
        "    return d4_tensor#torch.tensor(d4_tensor)\n",
        "\n",
        "    # Returns:\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "    pass\n",
        "\n",
        "\n",
        "def unit_test_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    # call your implemented \"im2col\" conv2D\n",
        "    y_im2col = conv2d_im2col(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # ground truth conv2D\n",
        "    y_gt = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "    \n",
        "    diff = (y_im2col - y_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of conv2d_im2col is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of conv2d_im2col is wrong!\")\n",
        "    \n",
        "    # call your implemented \"im2col\" conv2D\n",
        "    y_filter2row = conv2d_filter2row(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    \n",
        "    diff = (y_filter2row - y_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of conv2d_filter2row is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of conv2d_filter2row is wrong!\")\n",
        "\n",
        "\n",
        "unit_test_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ],
      "metadata": {
        "id": "-IRaDddEdgT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2790cfd0-3d91-484a-8ca5-75ee75ccc6c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of conv2d_im2col is correct!\n",
            "Your implementation of conv2d_filter2row is wrong!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.zeros((1,))\n",
        "# a"
      ],
      "metadata": {
        "id": "FN7Lj53zHCs9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ue67rcHR1x3I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 [20Pts] Implement the gradient of 2D convolution\n",
        "\n",
        "We now turn to the gradient of 2D convolution.\n",
        "In particular, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we can view the convolution (zero-padding and stride 1) as a function\n",
        "\\begin{align}\n",
        "    y = f(h, x),\n",
        "\\end{align}\n",
        "that would produce an output tensor $y$ with shape $B \\times D \\times H \\times W$.\n",
        "\n",
        "If we vectorize $x$, $h$, and $y$, then the Jacobian matrix $∇f = [\\frac{\\partial y}{\\partial h}, \\frac{\\partial y}{\\partial x}]$ would be of shape $BDHW \\times (DCKK + BCHW)$.\n",
        "In practice, we almost never need to compute the Jacobian matrix directly as it is unnecessary for back-propagation.\n",
        "Instead, we often need to compute the product between the transposed Jacobian and a vector (a.k.a. vector-Jacobian product), i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$.\n",
        "For example, the vector $v$ could the gradient of some loss $\\ell$ (scalar) w.r.t. the output above (i.e., $\\frac{\\partial \\ell}{\\partial y}$).\n",
        "\n",
        "**Task**: \n",
        "Given input images $x$, filters $h$, output $y$, and a vector $v$, implement the gradients ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ in the function below.\n",
        "\n",
        "**N.B.**: The function needs to return the gradients in the original shapes, i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ should have the same shape as $h$ ($D \\times C \\times K \\times K$) and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ should have the same shape as $x$ ($B \\times C \\times H \\times W$)."
      ],
      "metadata": {
        "id": "0l4zm6mEXCxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.zeros((2,3,4,5))\n",
        "# print(a.shape)\n",
        "# b = np.zeros((2,2,3))\n",
        "# b[0, 1, 2] = 1\n",
        "# print(b)\n",
        "\n",
        "# a_vector = a.flatten()\n",
        "# print(a_vector.shape)\n",
        "# b_vector = b.flatten()\n",
        "# print(b_vector.shape)\n",
        "# print(b_vector)"
      ],
      "metadata": {
        "id": "jucS7loHjnzI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.ones((2,3,4,5))\n",
        "# pad = 1\n",
        "# a_pad = np.pad(a, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        "# print(X_pad.shape)"
      ],
      "metadata": {
        "id": "N9PrTCkKuxxI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following functions\n",
        "def grad_conv2d(img, filter, out, grad_out, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "    #   grad_out: gradient w.r.t. output, shape B x D x H x W (e.g gradient of loss w.r.t. output of conv layer) = v\n",
        "\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "\n",
        "    B, C, H, W = img.shape\n",
        "    D, C, K, K = filter.shape\n",
        "    s = stride\n",
        "    pad = padding\n",
        "\n",
        "    #print(\"B, C, H, W, D is \", B, C, H, W, D)\n",
        "\n",
        "    #tensor to numpy\n",
        "    img = img #.detach().numpy()\n",
        "    filter = filter #.detach().numpy()\n",
        "    grad_out = grad_out #.detach().numpy()\n",
        "\n",
        "    grad_img = torch.zeros((B, C, H, W))\n",
        "    grad_filter = torch.zeros((D,C,K,K))\n",
        "\n",
        "    '''\n",
        "    convolution(not img2col, filter2row)\n",
        "    convolution between sliding window(KxK size)&filter\n",
        "    '''\n",
        "    #padded_img = np.pad(img, ((0,0), (0,0),(pad,pad), (pad,pad)), 'constant', constant_values = (0,0))\n",
        "    padded_img = padding_scratch4(img, pad) \n",
        "    #print(padded_img.shape)\n",
        "    #padded_grad_img = np.pad(grad_img, ((0,0), (0,0),(pad,pad), (pad,pad)), 'constant', constant_values = (0,0))\n",
        "    padded_grad_img = padding_scratch4(grad_img, pad)\n",
        "    #print(\"padded_img shape\",padded_img.shape)\n",
        "    \n",
        "    for i in range(B): #batchsize\n",
        "      #ith img from img, grad_img\n",
        "      pad_img_1 = padded_img[i] #(C, H+2p, W+2p)\n",
        "      pad_grad_img_1 = padded_grad_img[i] #(C, H+2p, W+2p)\n",
        "      for h in range(H): #height\n",
        "        for w in range(W): #width\n",
        "          for d in range(D): #output img's channel (#filters)\n",
        "            \n",
        "            #window (convolution w/ filter)\n",
        "            window_1 = pad_img_1[:,h:h+K, w:w+K]\n",
        "\n",
        "            #update gradients for the window and the filter\n",
        "            pad_grad_img_1[:, h:h+K, w:w+K] += filter[d,:,:,:] * grad_out[i,d,h,w]\n",
        "            # print(i, d, h, w)\n",
        "            # print(grad_filter[d, :, :, :])\n",
        "            # print(grad_out[i,d,h,w])\n",
        "            grad_filter[d,:, :, :]+= window_1*grad_out[i,d,h,w]\n",
        "      grad_img[i, :, :, :] = pad_grad_img_1[:, pad:-pad, pad:-pad]\n",
        "\n",
        "    # Returns:\n",
        "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
        "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
        "    return torch.tensor(grad_img), torch.tensor(grad_filter)\n",
        "    #pass\n",
        "\n",
        "\n",
        "def unit_test_grad_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    filter.requires_grad = True\n",
        "    img.requires_grad = True\n",
        "\n",
        "    ### ground truth conv2D\n",
        "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(img_out)\n",
        "\n",
        "    # call your implemented \"grad_conv2d\" function\n",
        "    grad_img, grad_filter = grad_conv2d(img, filter, img_out, v, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
        "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    diff = (grad_img - grad_img_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_img is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_img is wrong!\")\n",
        "\n",
        "    diff = (grad_filter - grad_filter_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter is wrong!\")        \n",
        "\n",
        "unit_test_grad_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ],
      "metadata": {
        "id": "Vtz-vk95d9WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dab8cc-bf39-4ccd-bf31-7fc5ea6ef226"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-cb11f016126d>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(grad_img), torch.tensor(grad_filter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of grad_img is correct!\n",
            "Your implementation of grad_filter is correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1.3 [10Pts]: Implement gradient checking via the finite difference approximation\n",
        "\n",
        "We verify the correctness of the implementation of gradient operators by calling PyTorch's autograd function.\n",
        "However, PyTorch's autograd function just calls the gradient operators implemented by the PyTorch team.\n",
        "How do they verify the correctness of their implementation?\n",
        "\n",
        "The answer is **finite difference approximation**.\n",
        "Following the setup in 1.2, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we have the convolution (zero-padding and stride 1)\n",
        "\\begin{align}\n",
        "    y = f(h, x).\n",
        "\\end{align}\n",
        "\n",
        "Again, mentally vectorize $x$ and $y$ would help us understand the math. \n",
        "Given any vector $v$ with the same shape as $y$, we are interested in computing ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$. \n",
        "These two gradients are equivalent to ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$ where\n",
        "\\begin{align}\n",
        "    \\ell(h, x) = y^{\\top}v = f(h, x)^{\\top} v.\n",
        "\\end{align}\n",
        "Note that here $\\ell$ becomes a scalar.\n",
        "Based on Talyor's theorem, we have\n",
        "\\begin{align}\n",
        "    d^{\\top} \\frac{\\partial \\ell}{\\partial h} = \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot d, x) - \\ell(h - \\epsilon \\cdot d, x)}{2 ϵ},\n",
        "\\end{align}\n",
        "where $d$ could be any direction vector and $ϵ$ is a scalar.\n",
        "For our purpose, we just need to set $d$ to be the unit vector to compute the per-dimension value of $\\frac{\\partial \\ell}{\\partial h}$. \n",
        "Specifically, if we set $d$ as the $i$-th unit vector $e_i$, i.e., $d[i] = 1$ and $d[j] = 0, \\forall j \\neq i$, we can then compute \n",
        "\\begin{align}\n",
        "    \\frac{\\partial \\ell}{\\partial h}[i] &= \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ} \\\\\n",
        "    & ≈ \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ}.\n",
        "\\end{align}\n",
        "\n",
        "**Task**: Implement the finite-difference based gradient checker for ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$. \n",
        "\n",
        "\n",
        "**N.B.**: For efficiency consideration in the unit test, you can use F.conv2d to compute the convolution in your implementation of *grad_checker*. This assignment is to let you understand how to implement finte-difference. But in pratice, if we want to verify our implementation of conv2d, then we should use our conv2d instead of F.conv2d from PyTorch.\n"
      ],
      "metadata": {
        "id": "QI9J5CDCUSCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following functions\n",
        "def grad_checker(img, filter, conv, grad_out, epsilon=1.0e-5, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   conv: convolution function\n",
        "    #   out: convoluted images, shape B x D x H x W (can get using, img, filter, conv)\n",
        "    #   grad_out: gradient w.r.t. output, shape B x D x H x W (loss w.r.t out)\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    B,C,H,W = img.shape\n",
        "    D,C,K,K = filter.shape\n",
        "\n",
        "    # for i in range(D*C*K*K):\n",
        "    #   ei = torch.zeros((D*C*K*K))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    grad_img = torch.zeros(B,C,H,W)\n",
        "    grad_filter = torch.zeros(D,C,K,K)\n",
        "    # Returns:\n",
        "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
        "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
        "    return grad_img, grad_filter\n",
        "    # pass\n",
        "\n",
        "\n",
        "def unit_test_grad_checker(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    epsilon = 1.0e-5\n",
        "    filter.requires_grad = True\n",
        "    img.requires_grad = True\n",
        "\n",
        "    ### ground truth conv2D\n",
        "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(img_out)\n",
        "\n",
        "    # call your implemented \"grad_checker\" function\n",
        "    grad_img, grad_filter = grad_checker(img, filter, F.conv2d, v, epsilon=epsilon, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
        "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    diff = (grad_img - grad_img_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_img is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_img is wrong!\")\n",
        "\n",
        "    diff = (grad_filter - grad_filter_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter is wrong!\")        \n",
        "\n",
        "unit_test_grad_checker(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ],
      "metadata": {
        "id": "qgeHgGn1JbBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ee5bf1-0087-4e69-d2be-6ca16315fd80"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of grad_img is wrong!\n",
            "Your implementation of grad_filter is wrong!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Q2 [5Pts]: Implement ReLU and its gradient\n",
        "\n",
        "**Task**: Implement ReLU operator, i.e., $f(x) = max(x, 0)$, and its gradient operator ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any given tensor $v$ that is of the same shape as $x$.\n",
        "\n",
        "**N.B.**: For simplicity, we can assume the input $x$ is of shape $B \\times C \\times H \\times W$ as before."
      ],
      "metadata": {
        "id": "rJ3DtRlwUlzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector = np.array([1, -1, 0, 1, 2, -3])\n",
        "# print(vector.shape)\n",
        "# for i in range(6):\n",
        "#   vector[i] = max(0, vector[i])\n",
        "# print(vector)"
      ],
      "metadata": {
        "id": "KsWng-17m_iu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.zeros((3,3))\n",
        "# a[0, :] = -1\n",
        "# a[1, :] = 0\n",
        "# a[2, :] = 2\n",
        "# print(a)\n",
        "# new_a = np.maximum(a, 0)\n",
        "# print(new_a)"
      ],
      "metadata": {
        "id": "iA6bo5CMqpW5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following functions\n",
        "def func_relu(x): #element-wise...?\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "     #get input shape\n",
        "  #   B, C, H, W = x.shape\n",
        "    #tensor_2_numpy_x = x.detach().numpy()\n",
        "    input2torchmat = torch.zeros((x.shape))\n",
        "    return torch.tensor(torch.maximum(x, input2torchmat))\n",
        "\n",
        "    # # Returns:\n",
        "    # #   y: output, shape B x C x H x W\n",
        "    # pass\n",
        "    \n",
        "\n",
        "def grad_relu(x, y, grad_out):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #   y: output, shape B x C x H x W\n",
        "    #   grad_out: gradient w.r.t. output y, shape B x D x H x W\n",
        "    #\n",
        "    bool = x > 0\n",
        "    #print(bool.shape)\n",
        "    grad_x = bool * grad_out\n",
        "\n",
        "    # Returns:\n",
        "    #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
        "    return grad_x\n",
        "    #pass\n",
        "\n",
        "\n",
        "def unit_test_relu(x):    \n",
        "    x.requires_grad = True\n",
        "    # print(x.shape)\n",
        "    # call your implemented \"func_relu\" function\n",
        "    y = func_relu(x)\n",
        "\n",
        "    # ground truth ReLU\n",
        "    y_gt = F.relu(x)\n",
        "\n",
        "    # print(y)\n",
        "    # print(y_gt)\n",
        "    diff = (y - y_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of func_relu is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of func_relu is wrong!\")\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(y)\n",
        "\n",
        "    # call your implemented \"grad_relu\" function\n",
        "    grad_x = grad_relu(x, y, v)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]    \n",
        "\n",
        "    diff = (grad_x - grad_x_gt).norm()    \n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_relu is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_relu is wrong!\")        \n",
        "\n",
        "unit_test_relu(torch.randn_like(img))"
      ],
      "metadata": {
        "id": "_ruiFoTsJb9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a212a6f6-0762-427a-96b9-48e04bc4216d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of func_relu is correct!\n",
            "Your implementation of grad_relu is correct!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-027ee0f4018e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(torch.maximum(x, input2torchmat))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Q3 [20Pts]: Implement Batch-normalization (BN) for convolution and its gradient\n",
        "\n",
        "Given a batch of input images $x$ with shape $B \\times C \\times H \\times W$, we compute a single mean and a single standard deviation per channel as below, \n",
        "\\begin{align}\n",
        "    \\mu[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} x[i, c, m, n] \\\\\n",
        "    \\sigma^2[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} (x[i, c, m, n] - \\mu[c])^2.\n",
        "\\end{align}\n",
        "Then we perform BN, $y = f(x, \\beta, \\gamma)$, as,\n",
        "\\begin{align}\n",
        "    y[i,c,m,n] &= \\gamma[c] \\frac{x[i,c,m,n] - \\mu[c]}{\\sqrt{\\sigma^2[c] + \\epsilon}} + \\beta[c],\n",
        "\\end{align}\n",
        "where $\\gamma$ and $\\beta$ are learnable parameters are of shape $C$.\n",
        "$ϵ$ is a constant.\n",
        "\n",
        "**Task**: For simplicity, we fix the learnable parameters as $\\gamma = 1$ and $\\beta = 0$.\n",
        "Implement BN for convolution and its gradient operators ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any $v$ that is compatible with the matrix multiplication. \n",
        "\n"
      ],
      "metadata": {
        "id": "jiVr7-xJU066"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C = 3\n",
        "# m = [0 for i in range(C)]\n",
        "# print(m)"
      ],
      "metadata": {
        "id": "uUbXUqF7tMxb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.zeros((5,2,3,3))\n",
        "# a[:,1:2,:,:].shape"
      ],
      "metadata": {
        "id": "tH-deACV1g1r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following functions\n",
        "def func_batch_norm(x, epsilon=1.0e-5):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #   epsilon: constant, scalar\n",
        "    #\n",
        "    B, C, H, W = x.shape\n",
        "    #print(B,C,H,W)\n",
        "    \n",
        "    x = x #.detach().numpy()\n",
        "    # print(x.shape)\n",
        "    y = torch.zeros((B, C, H, W))\n",
        "\n",
        "    #mean per channel\n",
        "    m = [0 for i in range(C)]\n",
        "\n",
        "    # sd per channel\n",
        "    sd = [0 for i in range(C)]\n",
        "\n",
        "    #get mean per channel\n",
        "    for c in range(C):\n",
        "      sum = 0\n",
        "      for b in range(B):\n",
        "        for h in range(H):\n",
        "          for w in range(W):\n",
        "            sum += x[b, c, h, w]\n",
        "      m[c] = sum / (B*H*W)\n",
        "    \n",
        "    # print(m)\n",
        "\n",
        "    #get std per channel\n",
        "    for c in range(C):\n",
        "      sum = 0\n",
        "      for b in range(B):\n",
        "        for h in range(H):\n",
        "          for w in range(W):\n",
        "            # print(x[b, c, h, w])\n",
        "            # print(m[c])\n",
        "            sum += (x[b, c, h, w] - m[c])**2\n",
        "      sd[c] = sum / (B*H*W)\n",
        "\n",
        "    # print(sd)\n",
        "\n",
        "    #perform BN\n",
        "    for b in range(B):\n",
        "      for c in range(C):\n",
        "        for h in range(H):\n",
        "          for w in range(W):\n",
        "            y[b,c,h,w] = (x[b,c,h,w] - m[c]) / (torch.sqrt(sd[c]+epsilon))\n",
        "\n",
        "    return y #torch.tensor(y)\n",
        "\n",
        "    # # Returns:\n",
        "    # #   y: output, shape B x C x H x W\n",
        "    # pass\n",
        "\n",
        "\n",
        "def grad_batch_norm(x, y, grad_out, epsilon=1.0e-5):\n",
        "    # ### Fill in this function ###\n",
        "    # # Args:\n",
        "    # #   x: input, shape B x C x H x W\n",
        "    # #   y: output, shape B x C x H x W\n",
        "    # #   grad_out: gradient w.r.t. output y, shape B x C x H x W\n",
        "    # #   epsilon: constant, scalar\n",
        "    # #\n",
        "\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    # #tensor to numpy\n",
        "    # x = x.detach().numpy()\n",
        "    # #y = y.detach().numpy()\n",
        "    # grad_out = grad_out.detach().numpy()\n",
        "    # #print(grad_out.shape) # (5,1,28,28)\n",
        "    # grad_x = np.zeros((B, C, H, W))\n",
        "    \n",
        "    # #mean per channel\n",
        "    # m = [0 for i in range(C)]\n",
        "\n",
        "    # # sd per channel\n",
        "    # sd = [0 for i in range(C)]\n",
        "\n",
        "    # #get mean per channel\n",
        "    # for c in range(C):\n",
        "    #   sum = 0\n",
        "    #   for b in range(B):\n",
        "    #     for h in range(H):\n",
        "    #       for w in range(W):\n",
        "    #         sum += x[b, c, h, w]\n",
        "    #   m[c] = sum / (B*H*W)\n",
        "    \n",
        "    # # print(m)\n",
        "\n",
        "    # #get std per channel\n",
        "    # for c in range(C):\n",
        "    #   sum = 0\n",
        "    #   for b in range(B):\n",
        "    #     for h in range(H):\n",
        "    #       for w in range(W):\n",
        "    #         # print(x[b, c, h, w])\n",
        "    #         # print(m[c])\n",
        "    #         sum += (x[b, c, h, w] - m[c])**2\n",
        "    #   sd[c] = sum / (B*H*W)\n",
        "\n",
        "    # #update gradient\n",
        "    # grad_loss_sd = np.zeros((B,C,H,W))\n",
        "    # for b in range(B):\n",
        "    #   for h in range(H):\n",
        "    #     for w in range(W):\n",
        "    #       for c in range(C):\n",
        "    #         for cc in range(C):\n",
        "    #           grad_loss_sd[b,c:c+1,h,w] += (grad_out[b,cc:cc+1,h,w]*(x[b,cc:cc+1,h,w]-m[c])*(-0.5)*((sd[c]+epsilon)**(-2/3)))\n",
        "            \n",
        "    # # for c in range(C):\n",
        "    # #   for cc in range(C):\n",
        "    # #     grad_loss_sd[:,c:c+1,:,:] += (grad_out[:,cc:cc+1,:,:]*(x[:,cc:cc+1,:,:]-m[c])*(-0.5)*((sd[c]+epsilon)**(-2/3)))\n",
        "\n",
        "    # grad_loss_mean = np.zeros((B,C,H,W))\n",
        "    # tmp1 = np.zeros((B,C,H,W))\n",
        "    # tmp2 = np.zeros((B,C,H,W))\n",
        "    # for b in range(B):\n",
        "    #   for h in range(H):\n",
        "    #     for w in range(W):\n",
        "    #       for c in range(C):\n",
        "    #         for cc in range(C):\n",
        "    #           #print(grad_out[:,cc,:,:].shape)\n",
        "    #           tmp1 += grad_out[b,cc:cc+1,h,w]*(-1)*((sd[c]+epsilon)**(-1/2))\n",
        "    #           tmp2 += -2*(x[b,cc:cc+1,h,w]-m[c])\n",
        "    #         grad_loss_mean[b,c:c+1,h,w] = tmp1 + grad_loss_sd[b,c:c+1,h,w]*tmp2/ C\n",
        "    \n",
        "    # # yj = np.zeros((B,1, H,W))\n",
        "    # # jeno = np.zeros((B,1, H,W))\n",
        "\n",
        "    # # for c in range(C):\n",
        "    # #   print(grad_out[:,c:c+1,:,:].shape)\n",
        "    # #   yj += grad_out[:,c:c+1,:,:]\n",
        "    # #   jeno += grad_out[:,c:c+1,:,:]*x[:,c:c+1,:,:]\n",
        "\n",
        "    # # for c in range(C):\n",
        "    # #   grad_x[:, c:c+1, :, :] = (m[c]*grad_out[:, c:c+1, :, :] - yj - x[:,c:c+1,:,:]*jeno)*(1/(m[c]*np.sqrt(sd[c]+epsilon)))\n",
        "    # # for i in range(B): #batchsize\n",
        "    # #   #ith img from img, grad_img\n",
        "    # #   #img_1 = x[i] #(C, H, W)\n",
        "    # #   for h in range(H): #height\n",
        "    # #     for w in range(W): #width\n",
        "    # #         for c in range(C): #img's channel  \n",
        "    # #           #update gradients for the y[i,c,m,n] \n",
        "    # #           yj += grad_out[i,c,h,w]\n",
        "    # #           jeno += x[i, c, h, w] * grad_out[i,c,h,w]\n",
        "    # #           grad_x[i,c, h, w] = \n",
        "    # for b in range(B):\n",
        "    #   for h in range(H):\n",
        "    #     for w in range(W):\n",
        "    #       for c in range(C):\n",
        "    #         grad_x[b,c:c+1,h,w] = grad_out[b,c:c+1,h,w]*((sd[c]+epsilon)**(-1/2)) + grad_loss_sd[b,c:c+1,h,w]*2*(x[b,c:c+1,h,w]-m[c])/C + grad_loss_mean[b,c:c+1,h,w]*1/C\n",
        "\n",
        "    # # Returns:\n",
        "    # #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
        "    #return torch.tensor(grad_x)\n",
        "    return torch.zeros((B,C,H,W))\n",
        "    #pass \n",
        "\n",
        "\n",
        "def unit_test_batch_norm(x):\n",
        "    x.requires_grad = True\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    # call your implemented \"func_batch_norm\" function\n",
        "    y = func_batch_norm(x, epsilon=epsilon)\n",
        "\n",
        "    # ground truth ReLU\n",
        "    BN_gt = nn.BatchNorm2d(x.shape[1], eps=epsilon, momentum=1.0, affine=False, track_running_stats=False)\n",
        "    # print(BN_gt)\n",
        "    y_gt = BN_gt(x)\n",
        "    # print(\"y_gt is \", y_gt)\n",
        "    # print(\"y is \", y)\n",
        "    diff = (y - y_gt).norm()  \n",
        "    # print(\"diff is \", diff)  \n",
        "    # print(1.0e-5)\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of func_batch_norm is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of func_batch_norm is wrong!\")\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(y)\n",
        "\n",
        "    #call your implemented \"grad_batch_norm\" function\n",
        "    grad_x = grad_batch_norm(x, y, v, epsilon=epsilon)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]  #Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
        "\n",
        "    diff = (grad_x - grad_x_gt).norm()\n",
        "    #print(diff)\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_batch_norm is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_batch_norm is wrong!\")\n",
        "\n",
        "unit_test_batch_norm(torch.randn_like(img))"
      ],
      "metadata": {
        "id": "3Sou_6LzJcxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b03e3c-529a-47c4-f17c-f7143072c691"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of func_batch_norm is correct!\n",
            "Your implementation of grad_batch_norm is wrong!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Q4 [15Pts]: Implement a simple CNN and back-propagation (BP)\n",
        "\n",
        "Now we are ready to build a deep CNN and learn it with back-propagation.\n",
        "In particular, let us build a simple CNN with following architecture:\n",
        "\n",
        "Conv $→$ BN $→$ ReLU $→$ Conv $→$ BN $→$ ReLU $→$ Linear.\n",
        "\n",
        "Here, for all layers, the convolutions are the same as before (i.e., kernel size $3 \\times 3$, zero-padding, number of filters $D = 2$, and stride 1), the BNs are without learnable $\\gamma$ and $\\beta$, and the last linear layer would map whatever input dimension to $10$ classes in MNIST. \n",
        "\n",
        "**Task**: Implement the above CNN, compute the cross-entropy loss, and compute gradient of the loss w.r.t. filter weights.\n",
        "\n",
        "**N.B.**: You can use F.cross_entropy provided by PyTorch. But for other operators like Conv, BN, and ReLU and their gradietns, you should use your previous implementations."
      ],
      "metadata": {
        "id": "2m-KuDI7U2zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## implement the following two functions\n",
        "def CNN(img, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter_1: filters at 1st layer, shape D x C x K x K \n",
        "    #   filter_2: filters at 2nd layer, shape D x C x K x K\n",
        "    #   weight: weights of linear readout layer, shape ? x 10\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   out: logits, shape B x 10\n",
        "    \n",
        "    #print(\"img shape is \", img.shape)\n",
        "    img = img#.detach().numpy()\n",
        "    filter_1 = filter_1#.detach().numpy()\n",
        "    filter_2 = filter_2#.detach().numpy()\n",
        "    ### 1st layer\n",
        "    conv1 = conv2d_im2col(img, filter_1, channel_size, num_filters, kernel_size, stride, padding)\n",
        "    #print(\"conv1 shape is \", conv1.shape)\n",
        "    BN1 = func_batch_norm(conv1, epsilon=1.0e-5)\n",
        "    #print(\"BN1 shape is \", BN1.shape)\n",
        "    ReLU1 = func_relu(BN1)\n",
        "    #print(\"RELU1 shape is \", ReLU1.shape)\n",
        "    \n",
        "    ### 2nd layer\n",
        "    conv2 = conv2d_im2col(ReLU1, filter_2, channel_size, num_filters, kernel_size, stride, padding)\n",
        "    #print(\"conv2 shape is \", conv2.shape)\n",
        "    BN2 = func_batch_norm(conv2, epsilon=1.0e-5)\n",
        "    #print(\"BN2 shape is \", BN2.shape)\n",
        "    ReLU2 = func_relu(BN2)\n",
        "    #print(\"RELU2 shape is \", ReLU2.shape) #RELU2 shape is  torch.Size([5, 2, 28, 28])\n",
        "\n",
        "    ### linear readout\n",
        "    weight = weight#.detach().numpy()\n",
        "    #print(\"weight shape is \", weight.shape)\n",
        "    flatten_relu2 = torch.reshape(ReLU2, (B,-1))#ReLU2.reshape(B,-1)\n",
        "    logits = torch.matmul(flatten_relu2, weight) #np.dot(flatten_relu2, weight) #\n",
        "    #print(\"logits shape is \",logits.shape)\n",
        "\n",
        "    return logits #torch.tensor(logits)\n",
        "    #pass\n",
        "\n",
        "\n",
        "def grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter_1: filters at 1st layer, shape D x C x K x K\n",
        "    #   filter_2: filters at 2nd layer, shape D x C x K x K\n",
        "    #   weight: weights of linear readout layer, shape ? x 10\n",
        "    #   grad_loss: gradient of loss w.r.t. logits, shape B x 10 loss/y\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   grad_filter_1: filters, shape D x C x K x K\n",
        "    #   grad_filter_2: filters, shape D x C x K x K\n",
        "    #   grad_weight: weight, shape ? x 10\n",
        "    \n",
        "    #CNN\n",
        "    img = img#.detach().numpy()\n",
        "    filter_1 = filter_1#.detach().numpy()\n",
        "    filter_2 = filter_2#.detach().numpy()\n",
        "    ### 1st layer\n",
        "    conv1 = conv2d_im2col(img, filter_1, channel_size, num_filters, kernel_size, stride, padding)\n",
        "    #print(\"conv1 shape is \", conv1.shape)\n",
        "    BN1 = func_batch_norm(conv1, epsilon=1.0e-5)\n",
        "    #print(\"BN1 shape is \", BN1.shape)\n",
        "    ReLU1 = func_relu(BN1)\n",
        "    #print(\"RELU1 shape is \", ReLU1.shape)\n",
        "    \n",
        "    ### 2nd layer\n",
        "    conv2 = conv2d_im2col(ReLU1, filter_2, channel_size, num_filters, kernel_size, stride, padding)\n",
        "    #print(\"conv2 shape is \", conv2.shape)\n",
        "    BN2 = func_batch_norm(conv2, epsilon=1.0e-5)\n",
        "    #print(\"BN2 shape is \", BN2.shape)\n",
        "    ReLU2 = func_relu(BN2)\n",
        "    #print(\"RELU2 shape is \", ReLU2.shape) #RELU2 shape is  torch.Size([5, 2, 28, 28])\n",
        "\n",
        "    ### linear readout\n",
        "    weight = weight#.detach().numpy()\n",
        "    #print(\"weight shape is \", weight.shape)\n",
        "    flatten_relu2 = torch.reshape(ReLU2,(B,-1))\n",
        "    logits = torch.matmul(flatten_relu2, weight) #\n",
        "    #print(\"logits shape is \",logits.shape)\n",
        "    ##\n",
        "\n",
        "    weight1shape, weight2shape = weight.shape\n",
        "    grad_filter_1 = torch.zeros((D,C,K,K))\n",
        "    grad_filter_2 = torch.zeros((D,C,K,K))\n",
        "    grad_weight = torch.zeros((weight1shape, weight2shape))\n",
        "    \n",
        "    # y = CNN(img, filter_1, filter_2, weight, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    # loss = F.cross_entropy(y, label).mean()\n",
        "\n",
        "    ### linear readout (grad_weight)\n",
        "    # print(\"flatten_relu2 shape is \", flatten_relu2.shape)\n",
        "    # print(\"grad_loss shape is \", grad_loss.shape)\n",
        "    # print(torch.transpose(flatten_relu2, 0,1).shape)\n",
        "    grad_weight = torch.matmul(torch.transpose(flatten_relu2,0,1),grad_loss)\n",
        "    print(grad_weight.shape)\n",
        "    ### 2nd layer (grad_filter2)\n",
        "\n",
        "\n",
        "    ### 1st layer (grad_filter1)\n",
        "\n",
        "    \n",
        "\n",
        "    return  torch.tensor(grad_filter_1),  torch.tensor(grad_filter_2),  torch.tensor(grad_weight)\n",
        "    \n",
        "    #pass\n",
        "\n",
        "\n",
        "def unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    # call your implemented \"CNN\"\n",
        "    img.requires_grad_()\n",
        "    filter_1.requires_grad_()\n",
        "    filter_2.requires_grad_()\n",
        "    weight.requires_grad_()\n",
        "    y = CNN(img, filter_1, filter_2, weight, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    #print(\"y shape is\", y.shape) #remove before submit. It should be (10, )\n",
        "    y.requires_grad_()\n",
        "\n",
        "    # compute loss function\n",
        "    #print(\"label is\", label)\n",
        "    loss = F.cross_entropy(y, label).mean()\n",
        "    loss.requires_grad_()\n",
        "\n",
        "    # compute gradient of loss w.r.t. logits\n",
        "    grad_loss = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
        "\n",
        "    # call your implemented \"grad_batch_norm\" function\n",
        "    grad_filter_1, grad_filter_2, grad_weight = grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    #print(grad_weight.shape)\n",
        "    # compute ground-truth gradients\n",
        "    grad_filter_1_gt = torch.autograd.grad(loss, filter_1, retain_graph=True)[0]\n",
        "    grad_filter_2_gt = torch.autograd.grad(loss, filter_2, retain_graph=True)[0]\n",
        "    grad_weight_gt = torch.autograd.grad(loss, weight, retain_graph=True)[0]\n",
        "\n",
        "    diff = (grad_filter_1 - grad_filter_1_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter_1 is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter_1 is wrong!\")\n",
        "\n",
        "    diff = (grad_filter_2 - grad_filter_2_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter_2 is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter_2 is wrong!\")\n",
        "\n",
        "    diff = (grad_weight - grad_weight_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_weight is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_weight is wrong!\")                \n",
        "\n",
        "\n",
        "filter_1 = torch.randn(D, C, K, K) # filter shape: D x C x K x K\n",
        "filter_2 = torch.randn(D, D, K, K) # filter shape: D x C x K x K\n",
        "\n",
        "### compute the correct shape and then replace None with it ###\n",
        "weight = torch.randn(2*28*28, 10) # weight of the last linear layer\n",
        "\n",
        "unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ],
      "metadata": {
        "id": "OnpM4Yj_JdVO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "b4977313-a872-4965-819b-abeaf93f08c5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-027ee0f4018e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(torch.maximum(x, input2torchmat))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1568, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-48534e616943>:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return  torch.tensor(grad_filter_1),  torch.tensor(grad_filter_2),  torch.tensor(grad_weight)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-48534e616943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# weight of the last linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0munit_test_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-48534e616943>\u001b[0m in \u001b[0;36munit_test_CNN\u001b[0;34m(img, label, filter_1, filter_2, weight, channel_size, num_filters, kernel_size, stride, padding)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m#print(grad_weight.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# compute ground-truth gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mgrad_filter_1_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mgrad_filter_2_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mgrad_weight_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXm3kHacBflK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}