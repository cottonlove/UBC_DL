{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fJmxtD-kg8KE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Eritefi9hPgk"
      },
      "outputs": [],
      "source": [
        "class SubstringDataset(Dataset):\n",
        "    LETTERS = list('cpen')\n",
        "\n",
        "    def __init__(self, seed, dataset_size, str_len=20):\n",
        "        super().__init__()\n",
        "        self.str_len = str_len\n",
        "        self.dataset_size = dataset_size\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.strings, self.labels = self._create_dataset()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strings[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def _create_dataset(self):\n",
        "        strings, labels = [], []\n",
        "        for i in range(self.dataset_size):\n",
        "            label = i%2\n",
        "            string = self._generate_random_string(bool(label))\n",
        "            strings.append(string)\n",
        "            labels.append(label)\n",
        "        return strings, labels\n",
        "    \n",
        "    def _generate_random_string(self, has_cpen):\n",
        "        while True:\n",
        "            st = ''.join(self.rng.choice(SubstringDataset.LETTERS, size=self.str_len))\n",
        "            if ('cpen' in st) == has_cpen:\n",
        "                return st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGSrkj4m9OoL"
      },
      "outputs": [],
      "source": [
        "# vocab = {\n",
        "#             '[CLS]': 0,\n",
        "#             'c': 1,\n",
        "#             'p': 2,\n",
        "#             'e': 3,\n",
        "#             'n': 4,\n",
        "#         }\n",
        "# char_list = ['c', 'c', 'p']\n",
        "# for i in range(len(char_list)):\n",
        "#   char_list[i] = vocab[char_list[i]]\n",
        "# char_list.insert(0, 0)\n",
        "# print(char_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVEGOAauEU_e"
      },
      "outputs": [],
      "source": [
        "# string = 'cccp'\n",
        "# x = list(string)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmzfrxekClOh"
      },
      "outputs": [],
      "source": [
        "# tokenized_string = []\n",
        "# for c in char_list:\n",
        "#   arr = list(np.zeros(len(vocab), dtype = int))\n",
        "#   arr[c] = 1\n",
        "#   tokenized_string.append(arr)\n",
        "\n",
        "# print(tokenized_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AY91aQytmtqK"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = {\n",
        "            '[CLS]': 0,\n",
        "            'c': 1,\n",
        "            'p': 2,\n",
        "            'e': 3,\n",
        "            'n': 4,\n",
        "        }\n",
        "\n",
        "\n",
        "    def tokenize_string(self, string, add_cls_token=True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenize the input string according to the above vocab\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #1.splits the string and returns a list of characters(tokens)\n",
        "        char_list = list(string)\n",
        "        #print(char_list) ## remove before submit\n",
        "        #1-1. change char into num in vocab\n",
        "        for i in range(len(char_list)):\n",
        "          char_list[i] = self.vocab[char_list[i]]\n",
        "        #2. add a [CLS] token to the beginning of the list\n",
        "        if (add_cls_token == True): #add [CLS]\n",
        "          char_list.insert(0, 0)\n",
        "        \n",
        "        #3. convert each token into a one-hot vector and return resulting matrix\n",
        "        tokenized_string = torch.zeros((len(char_list),len(self.vocab)))\n",
        "        for i in range(len(char_list)):\n",
        "          arr = list(torch.zeros(len(self.vocab)))\n",
        "          arr[char_list[i]] = 1\n",
        "          tokenized_string[i,:] = torch.tensor(arr)\n",
        "        # print(tokenized_string.shape)\n",
        "        # print(tokenized_string)\n",
        "        #tokenized_string = None #\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return tokenized_string\n",
        "\n",
        "    def tokenize_string_batch(self, strings, add_cls_token=True):\n",
        "        X = []\n",
        "        for s in strings:\n",
        "            X.append(self.tokenize_string(s, add_cls_token=add_cls_token))\n",
        "        return torch.stack(X, dim=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwhu1_9qxpzg"
      },
      "outputs": [],
      "source": [
        "# W = torch.empty(3,4)\n",
        "# print(W.shape)\n",
        "\n",
        "# p_W = W[0:2, :]\n",
        "# print(p_W.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q92iTaQL2zC_"
      },
      "outputs": [],
      "source": [
        "# H = [torch.empty((3, 4)) for _ in range(2)]\n",
        "# print(H)\n",
        "# print(H[0].shape)\n",
        "# print(H[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsfm88664Sfh"
      },
      "outputs": [],
      "source": [
        "# a = torch.ones((2,3))\n",
        "# b = torch.ones((2,3))\n",
        "# c = torch.concat((a,b),0)\n",
        "# print(c.shape)\n",
        "# c = torch.concat((a,b),1)\n",
        "# print(c.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFCWjnkw_l_I"
      },
      "outputs": [],
      "source": [
        "# a = torch.randn(2, 3)\n",
        "# print(a)\n",
        "# soft = F.softmax(a, dim = 1)\n",
        "# print(soft)\n",
        "\n",
        "# soft = F.softmax(a, dim = 0)\n",
        "# print(soft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOlQWSaaB9BM"
      },
      "outputs": [],
      "source": [
        "# mAX_LEN = 2\n",
        "# a = torch.empty((2*mAX_LEN+1, ))\n",
        "# print(a)\n",
        "# print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiASWG3RG79X"
      },
      "outputs": [],
      "source": [
        "# for i in range(2):\n",
        "#   print(i)\n",
        "\n",
        "# t = [1,2,3,4,5]\n",
        "# print(t[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AG_mI9VSj0XM"
      },
      "outputs": [],
      "source": [
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    MAX_LEN = 256 # N(length of Sequence) <= MAX_LEN\n",
        "    def __init__(self, d_model): #d_model: the number of expected features in the encoder/decoder inputs (default=512)\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty((self.MAX_LEN, d_model))) \n",
        "        #i-th row: learnable vector corresponding to the i-th position in a sequence\n",
        "        nn.init.normal_(self.W)\n",
        "    \"\"\"This module applies positional encoding to a sequence \n",
        "    by element-wise adding rows of this matrix to their corresponding position in the input.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D # N: length of Sequence. B: Batch size. D: After tokenizer, each char becomes D-dimensional one-hot vector\n",
        "        returns:\n",
        "            out: shape B x N x D # D = d_model...?\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        # print(\"AbsolutePositional Encoding\")\n",
        "        # print(\"W shape is \", self.W.shape)\n",
        "        # print(x.shape)\n",
        "        B,N,D = x.shape\n",
        "        out = torch.zeros_like(x)\n",
        "\n",
        "        part_W = self.W[0:N, :]\n",
        "        for i in range(B):\n",
        "          out[i, :, :] = torch.add(part_W,x[i, :, :])\n",
        "\n",
        "        # out = None\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "\n",
        "    def __init__(self, d_model, n_heads, rpe):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"Number of heads must divide number of dimensions\"\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_model // n_heads\n",
        "        self.rpe = rpe #boolean \n",
        "        self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wk = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wv = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wo = nn.Parameter(torch.empty((d_model, d_model)))\n",
        "       \n",
        "        if rpe:\n",
        "            # -MAX_LEN, -MAX_LEN+1, ..., -1, 0, 1, ..., MAX_LEN-1, MAXLEN\n",
        "            self.rpe_w = nn.ParameterList([nn.Parameter(torch.empty((2*self.MAX_LEN+1, ))) for _ in range(n_heads)])\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            nn.init.xavier_normal_(self.Wk[h])\n",
        "            nn.init.xavier_normal_(self.Wq[h])\n",
        "            nn.init.xavier_normal_(self.Wv[h])\n",
        "            if rpe:\n",
        "                nn.init.normal_(self.rpe_w[h])\n",
        "        nn.init.xavier_normal_(self.Wo)\n",
        "\n",
        "    def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            key: shape B x N x D\n",
        "            query: shape B x N x D\n",
        "            value: shape B x N x D\n",
        "        return:\n",
        "            out: shape B x N x D\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        B,N,D = key.shape\n",
        "        out = torch.zeros_like(key)\n",
        "        tmp = torch.zeros_like(key)\n",
        "        m = self.MAX_LEN\n",
        "\n",
        "        # print(self.d_h) #32 (w/o RPE)\n",
        "        # print(N) #12 (w/o RPE)\n",
        "        # nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        #H = nn.ParameterList([nn.Parameter(torch.empty((D, self.d_h))) for _ in range(self.n_heads)])\n",
        "        H = [torch.empty((D,self.d_h)).to(key.device) for _ in range(self.n_heads)]\n",
        "        #print(H[0].shape)\n",
        "        #H = H.to(key.device)\n",
        "        #print(\"H[0] \", H[0].is_cuda)\n",
        "\n",
        "        # TO DO for RPE\n",
        "        #print(\"Wq[0]\", self.Wq[0].is_cuda)\n",
        "        #self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        #M = nn.ParameterList([nn.Parameter(torch.empty((N,N))) for _ in range(self.n_heads)]) #list of matrix\n",
        "        M = [torch.zeros((N,N)).to(key.device) for _ in range(self.n_heads)]\n",
        "        #M = M.to(key.device)\n",
        "        # for h in range(self.n_heads):\n",
        "        #     nn.init.xavier_normal_(M[h])\n",
        "        #print(\"M[0] \", M[0].is_cuda)\n",
        "        if self.rpe:\n",
        "          for h in range(self.n_heads): #h: head index number\n",
        "            degree_freedom = self.rpe_w[h][m-(N-1):m+(N-1)+1] #(indexing. 2N-1ê°œ)\n",
        "            #Use degree_freedom list to make Toeplitz matrix\n",
        "            for i in range(N):\n",
        "              temp_vec = degree_freedom[(N-1-i):(N-1-i+N)]\n",
        "              M[h][i,:] = temp_vec\n",
        "       \n",
        "        # print(self.rpe)\n",
        "\n",
        "        for b in range(B):\n",
        "          for h in range(self.n_heads):\n",
        "            XWq = torch.matmul(query[b,:,:],  self.Wq[h])\n",
        "            #print(\"XWq shape is \", XWq.shape) #(n, self.d_h) = (12, 32)\n",
        "            XWk_t = torch.transpose(torch.matmul(key[b,:,:], self.Wk[h]),0,1)\n",
        "            #print(\"XWk_t shape is \", XWk_t.shape) #(self.d_h, n) = (32, 12)\n",
        "            matmul_XWq_XWk_t = torch.matmul(XWq, XWk_t)\n",
        "            #before softmax, M[h] element-wise addition \n",
        "            # print(\"matmul_XWq_XWk_t \", matmul_XWq_XWk_t.is_cuda)\n",
        "            # print(\"M[h] \", M[h].is_cuda)\n",
        "            matmul_XWq_XWk_t = matmul_XWq_XWk_t + M[h]\n",
        "            #print(\"matmul_XWq_XWk_t shape is \", matmul_XWq_XWk_t.shape) #(n, n) = (12,12)\n",
        "            div_root_dh = matmul_XWq_XWk_t / math.sqrt(self.d_h)\n",
        "            #print(\"div_root_dh shape is \", div_root_dh.shape) #(n, n) = (12,12)\n",
        "            soft = F.softmax(div_root_dh, dim = 1)\n",
        "            #print(\"soft shape is \", soft.shape) # (n, n) = (12,12)\n",
        "            XWv = torch.matmul(value[b,:,:], self.Wv[h])\n",
        "            #print(\"XWv shape is \", XWv.shape)  #(n, self.d_h)  = (12, 32)\n",
        "            H[h] = torch.matmul(soft, XWv)\n",
        "            #print(\" H[h] shape is \",  H[h].shape) #(n, self.d_h)  = (12, 32)\n",
        "          #print(H[0].shape) # n, d_h = 12, 32\n",
        "          out[b,:,:] = torch.concat([H[h]for h in range(self.n_heads)],1)\n",
        "          out[b,:,:] = torch.matmul(out[b,:,:].clone(),self.Wo)\n",
        "        # out = None\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4iCqSqRDsGw3"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module): #transformer encoder layer\n",
        "    def __init__(self, d_model: int, n_heads: int, prenorm: bool, rpe: bool):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.prenorm = prenorm #boolean\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, rpe=rpe)\n",
        "        self.fc_W1 = nn.Parameter(torch.empty((d_model, 4*d_model)))\n",
        "        self.fc_W2 = nn.Parameter(torch.empty((4*d_model, d_model)))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        nn.init.xavier_normal_(self.fc_W1)\n",
        "        nn.init.xavier_normal_(self.fc_W2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D (B: batch_size, N: length of sequence, D = d_model )\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        B,N,D = x.shape\n",
        "        # print(\"transformer layer\")\n",
        "        # print(x.is_cuda)\n",
        "\n",
        "        if self.prenorm: #prenorm\n",
        "          after_ln1 = self.ln1(x) #(B,N,D)\n",
        "          mha_result = self.attention(after_ln1, after_ln1, after_ln1) #Xk=Xq=Xv=X #(B,N,D)\n",
        "          add_1 = x + mha_result #(B,N,D)\n",
        "\n",
        "          after_ln2 = self.ln2(add_1) #(B,N,D)\n",
        "\n",
        "          # after_fc1 = torch.ones(B,N,4*D)\n",
        "          # after_fc2 = torch.ones(B, N, D)\n",
        "\n",
        "          # for b in range(B):\n",
        "          #   after_fc1[b,:,:] = torch.matmul(after_ln2[b,:,:],  self.fc_W1)\n",
        "          #   after_relu = self.relu(after_fc1[b,:,:])\n",
        "          #   after_fc2[b,:,:] = torch.matmul(after_relu, self.fc_W2)\n",
        "          \n",
        "          ###\n",
        "          after_fc1 = torch.matmul(after_ln2,  self.fc_W1)\n",
        "          after_relu = self.relu(after_fc1)\n",
        "          after_fc2 = torch.matmul(after_relu, self.fc_W2)\n",
        "          ###\n",
        "\n",
        "          add_2 = add_1 + after_fc2\n",
        "          out = add_2\n",
        "\n",
        "\n",
        "        else: #postnorm\n",
        "          mha_result = self.attention(x, x, x) #Xk=Xq=Xv=X #(B,N,D)\n",
        "          add_1 = x + mha_result\n",
        "          after_ln1 = self.ln1(add_1)\n",
        "\n",
        "          # after_fc1 = torch.ones(B,N,4*D)\n",
        "          # after_fc2 = torch.ones(B, N, D)\n",
        "\n",
        "          # for b in range(B):\n",
        "          #   after_fc1[b,:,:] = torch.matmul(after_ln1[b,:,:],  self.fc_W1)\n",
        "          #   after_relu = self.relu(after_fc1[b,:,:])\n",
        "          #   after_fc2[b,:,:] = torch.matmul(after_relu, self.fc_W2)\n",
        "\n",
        "          ####\n",
        "          after_fc1= torch.matmul(after_ln1,  self.fc_W1)\n",
        "          after_relu = self.relu(after_fc1)\n",
        "          after_fc2 = torch.matmul(after_relu, self.fc_W2)\n",
        "\n",
        "\n",
        "          ###\n",
        "          \n",
        "          add_2 = after_ln1 + after_fc2\n",
        "          after_ln2 = self.ln2(add_2)\n",
        "          out = after_ln2\n",
        "      \n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l90s0uXUWSzB"
      },
      "outputs": [],
      "source": [
        "# a = torch.zeros((1,2,3))\n",
        "# b = torch.zeros((2,2,3))\n",
        "\n",
        "# print(a[0, :, :].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IxPXjEj1ydLf"
      },
      "outputs": [],
      "source": [
        "class ModelConfig:\n",
        "    n_layers = 4\n",
        "    input_dim = 5\n",
        "    d_model = 256\n",
        "    n_heads = 4\n",
        "    prenorm = True\n",
        "    pos_enc_type = 'ape' # 'ape': Abosolute Pos. Enc., 'rpe': Relative Pos. Enc.\n",
        "    output_dim = 1 # Binary output: 0: invalid, 1: valid\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class TransformerModel(nn.Module): #In this assignment, you only have TransformerEncoder (w/ just encoder, decoder) So, technically, you are still implementing an actual transformer, but a special variant..\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.enc_W = nn.Parameter(torch.empty((cfg.input_dim, cfg.d_model)))\n",
        "        if cfg.pos_enc_type == 'ape':\n",
        "            self.ape = AbsolutePositionalEncoding(d_model=cfg.d_model)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model=cfg.d_model, n_heads=cfg.n_heads, prenorm=cfg.prenorm, rpe=cfg.pos_enc_type == 'rpe') for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "        self.dec_W = nn.Parameter(torch.empty((cfg.d_model, cfg.output_dim)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.enc_W)\n",
        "        nn.init.xavier_normal_(self.dec_W)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D_in\n",
        "        returns:\n",
        "            out: shape B x N x D_out\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        # print(\"transformer model\")\n",
        "        # print(x.is_cuda)\n",
        "        #print(\"x is\", x)\n",
        "        B,N,D_in = x.shape\n",
        "        D_out = self.cfg.output_dim\n",
        "        D = self.cfg.d_model\n",
        "        \n",
        "        after_encoder = torch.matmul(x, self.enc_W)\n",
        "        # print(\"after_encoder\")\n",
        "        # print(after_encoder.is_cuda)\n",
        "        if self.cfg.pos_enc_type == 'ape':\n",
        "          after_encoder = self.ape(after_encoder)\n",
        "          # print(\"ape\")\n",
        "          # print(after_encoder.is_cuda)\n",
        "\n",
        "        ###\n",
        "        # after_1 = self.transformer_layers[0](after_encoder)\n",
        "        # after_2 = self.transformer_layers[1](after_1)\n",
        "        # after_3 = self.transformer_layers[2](after_2)\n",
        "        # after_4 = self.transformer_layers[3](after_3)\n",
        "\n",
        "        # after_decoder = torch.zeros(B,N,D_out)\n",
        "        # for b in range(B):\n",
        "        #   after_decoder[b,:,:] = torch.matmul(after_4[b,:,:], self.dec_W)\n",
        "        ###\n",
        "        \n",
        "        for i in range(self.cfg.n_layers):\n",
        "          after_encoder = self.transformer_layers[i](after_encoder)\n",
        "        \n",
        "        after_decoder = torch.matmul(after_encoder, self.dec_W)\n",
        "\n",
        "        out = after_decoder\n",
        "\n",
        "        # print(\"afterdecoder cuda\")\n",
        "        # print(after_decoder.is_cuda)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k32Ps5WS9rg-"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class CustomScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, total_steps, warmup_steps=1000):\n",
        "        self.total_steps = total_steps #maxSTEP\n",
        "        self.warmup_steps = warmup_steps #warmupSTEP\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the custom scheduler with warmup and cooldown\n",
        "        Hint: self.last_epoch contains the current step number\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        # print(self.optimizer)\n",
        "        # print(self.last_epoch)\n",
        "        # base_lr = self.optimizer.param_groups[0]['initial_lr']\n",
        "        #print(\"base lr is \", self.optimizer.param_groups[0]['initial_lr'])\n",
        "\n",
        "        current_step = self.last_epoch\n",
        "        if (current_step <= self.warmup_steps): #increase (warmup)\n",
        "          mult_factor = current_step/self.warmup_steps\n",
        "          \n",
        "        else: #decrease(cooldown)\n",
        "          mult_factor = ((-1)/(self.total_steps - self.warmup_steps)) * current_step + self.total_steps/(self.total_steps - self.warmup_steps)\n",
        "          \n",
        "        \n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return [group['initial_lr'] * mult_factor for group in self.optimizer.param_groups]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HmjFKAXcyeZm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainerConfig:\n",
        "    lr = 0.003\n",
        "    train_steps = 5000\n",
        "    batch_size = 256\n",
        "    evaluate_every = 100\n",
        "    device = 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, cfg: TrainerConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = cfg.device\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        ##\n",
        "        # for param in self.model.parameters():\n",
        "        #   print(param, param.requires_grad)\n",
        "        ##\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
        "        scheduler = CustomScheduler(optimizer, self.cfg.train_steps)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.cfg.batch_size)\n",
        "        # with torch.autograd.detect_anomaly():\n",
        "        for step in range(self.cfg.train_steps):\n",
        "            self.model.train()\n",
        "            batch = next(iter(train_dataloader))\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, _ = self.compute_batch_loss_acc(x, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            if step % self.cfg.evaluate_every == 0:\n",
        "                val_loss, val_acc = self.evaluate_dataset(val_dataset)\n",
        "                print(f\"Step {step}: Train Loss={loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_acc}\")\n",
        "\n",
        "    def compute_batch_loss_acc(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss and accuracy of the model on batch (x, y)\n",
        "        args:\n",
        "            x: B x N x D_in\n",
        "            y: B\n",
        "        return:\n",
        "            loss, accuracy\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        # print(type(x))\n",
        "        x_cuda = x.to(self.device)\n",
        "        y_cuda = y.to(self.device)\n",
        "        criterion = nn.BCELoss()\n",
        "        #criterion = nn.BCEWithLogitsLoss() #BCEWithLogitsLoss BCELoss\n",
        "        outputs = self.model(x_cuda) #outputs: BxNxD_out\n",
        "        # print(\"outputs\")\n",
        "        # print(outputs.requires_grad)\n",
        "        cls = outputs[:,0,0]#BxD_out = B*1\n",
        "        cls = torch.sigmoid(cls) #probability of class1(invalid) (Binary output: 0: invalid, 1: valid)\n",
        "        # print(cls)\n",
        "        # print(\"cls\", cls)\n",
        "        # print(\"y_cuda\", y_cuda)\n",
        "        #cls = cls.type(torch.float32)\n",
        "        y_cuda = y_cuda.type(torch.float32)\n",
        "        \n",
        "        loss = criterion(cls, y_cuda)\n",
        "        # loss.requires_grad = True\n",
        "        #loss = loss.type(torch.long)\n",
        "        # print(\"loss\", loss)\n",
        "        # print(loss.requires_grad)\n",
        "\n",
        "        correct_pred = torch.tensor([0.0])\n",
        "        #print(\"len(cls) is \", len(cls))\n",
        "        for i in range(len(cls)):\n",
        "          pred = 1 if cls[i] > 0.5 else 0 #threshold = 0.5\n",
        "          if y[i] == pred:\n",
        "            correct_pred = correct_pred + 1\n",
        "          else:\n",
        "            correct_pred = correct_pred +0\n",
        "        acc = correct_pred / len(cls)\n",
        "        # acc = acc.type(torch.long)\n",
        "        # loss, acc = torch.tensor([1.0]), torch.tensor([0.0])\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return loss, acc\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def evaluate_dataset(self, dataset):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, shuffle=False, batch_size=self.cfg.batch_size)\n",
        "        final_loss, final_acc = 0.0, 0.0\n",
        "        for batch in dataloader:\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "            loss, acc = self.compute_batch_loss_acc(x, y)\n",
        "            final_loss += loss.item() * x.size(0)\n",
        "            final_acc += acc.item() * x.size(0)\n",
        "        return final_loss / len(dataset), final_acc / len(dataset)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiO2tIbwpmCY"
      },
      "outputs": [],
      "source": [
        "# #dataset\n",
        "# train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=16)\n",
        "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=256)\n",
        "# batch = next(iter(train_dataloader))\n",
        "# strings, y = batch\n",
        "# print(y) #0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b5zfy4SVFy0V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In case you were not successful in implementing some of the above classes,\n",
        "you may reimplement them using pytorch available nn Modules here to receive the marks for part 1.8\n",
        "If your implementation of the previous parts is correct, leave this block empty.\n",
        "START BLOCK\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "END BLOCK\n",
        "\"\"\"\n",
        "def run_transformer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    #print(\"device is \", device) # remove b/f submit\n",
        "    model = TransformerModel(ModelConfig())\n",
        "    trainer = Trainer(model, TrainerConfig(device=device))\n",
        "    parantheses_size=16\n",
        "    print(\"Creating datasets.\")\n",
        "    train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=parantheses_size)\n",
        "    val_dataset = SubstringDataset(seed=2, dataset_size=1_000, str_len=parantheses_size)\n",
        "    test_dataset = SubstringDataset(seed=3, dataset_size=1_000, str_len=parantheses_size)\n",
        "\n",
        "    print(\"Training the model.\")\n",
        "    trainer.train(train_dataset, val_dataset)\n",
        "    test_loss, test_acc = trainer.evaluate_dataset(test_dataset)\n",
        "    print(f\"Final Test Accuracy={test_acc}, Test Loss={test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhAUyeO5F27T",
        "outputId": "28f970f1-e93c-4fe1-f296-41803d654042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets.\n",
            "Training the model.\n",
            "Step 0: Train Loss=2.958362102508545, Val Loss: 2.709890213012695, Val Accuracy: 0.5\n",
            "Step 100: Train Loss=0.6839382648468018, Val Loss: 0.7943038592338562, Val Accuracy: 0.5\n",
            "Step 200: Train Loss=0.8427833318710327, Val Loss: 0.8729402265548706, Val Accuracy: 0.5\n",
            "Step 300: Train Loss=0.6841714978218079, Val Loss: 0.693966637134552, Val Accuracy: 0.5420000052452087\n",
            "Step 400: Train Loss=0.6919990181922913, Val Loss: 0.7365768885612488, Val Accuracy: 0.5\n",
            "Step 500: Train Loss=0.7070266008377075, Val Loss: 0.6892928490638733, Val Accuracy: 0.5320000009536743\n",
            "Step 600: Train Loss=0.6800819635391235, Val Loss: 0.6869859461784362, Val Accuracy: 0.5440000052452088\n",
            "Step 700: Train Loss=0.7146021723747253, Val Loss: 0.7387604341506958, Val Accuracy: 0.5209999976158142\n",
            "Step 800: Train Loss=0.8186905384063721, Val Loss: 0.8280617690086365, Val Accuracy: 0.5\n",
            "Step 900: Train Loss=0.49571675062179565, Val Loss: 0.6330121359825134, Val Accuracy: 0.6780000052452088\n",
            "Step 1000: Train Loss=0.502118706703186, Val Loss: 0.4496112010478973, Val Accuracy: 0.7930000019073487\n",
            "Step 1100: Train Loss=0.39190542697906494, Val Loss: 0.3936839699745178, Val Accuracy: 0.8359999933242798\n",
            "Step 1200: Train Loss=0.21072886884212494, Val Loss: 0.22724798226356507, Val Accuracy: 0.9100000066757202\n",
            "Step 1300: Train Loss=0.25297459959983826, Val Loss: 0.24569872903823853, Val Accuracy: 0.8979999985694885\n",
            "Step 1400: Train Loss=0.094533771276474, Val Loss: 0.13567834663391112, Val Accuracy: 0.9520000004768372\n",
            "Step 1500: Train Loss=0.15163889527320862, Val Loss: 0.12297957849502564, Val Accuracy: 0.9509999957084656\n",
            "Step 1600: Train Loss=0.05983811244368553, Val Loss: 0.14770622074604034, Val Accuracy: 0.9449999947547912\n",
            "Step 1700: Train Loss=0.019683267921209335, Val Loss: 0.02074483336508274, Val Accuracy: 0.9920000023841858\n",
            "Step 1800: Train Loss=0.08013969659805298, Val Loss: 0.12391219413280487, Val Accuracy: 0.9580000038146973\n",
            "Step 1900: Train Loss=0.004490870051085949, Val Loss: 0.015123069860041142, Val Accuracy: 0.993999994277954\n",
            "Step 2000: Train Loss=0.02544415555894375, Val Loss: 0.05784185218811035, Val Accuracy: 0.9789999990463257\n",
            "Step 2100: Train Loss=0.06621046364307404, Val Loss: 0.037769957691431044, Val Accuracy: 0.9879999966621399\n",
            "Step 2200: Train Loss=0.05683672055602074, Val Loss: 0.04867989102005959, Val Accuracy: 0.9869999966621399\n",
            "Step 2300: Train Loss=0.12741701304912567, Val Loss: 0.1764664944410324, Val Accuracy: 0.9449999995231628\n",
            "Step 2400: Train Loss=0.04985269531607628, Val Loss: 0.03407074508070946, Val Accuracy: 0.9849999990463257\n",
            "Step 2500: Train Loss=0.007938873954117298, Val Loss: 0.03736019653081894, Val Accuracy: 0.9889999990463256\n",
            "Step 2600: Train Loss=0.06646799296140671, Val Loss: 0.09062267357110977, Val Accuracy: 0.9639999980926514\n",
            "Step 2700: Train Loss=0.002845687558874488, Val Loss: 0.016999090857803822, Val Accuracy: 0.995999994277954\n",
            "Step 2800: Train Loss=0.00444470439106226, Val Loss: 0.02811239269375801, Val Accuracy: 0.9930000023841858\n",
            "Step 2900: Train Loss=0.01641995832324028, Val Loss: 0.025804773569107056, Val Accuracy: 0.9900000023841858\n",
            "Step 3000: Train Loss=0.0015994921559467912, Val Loss: 0.012356872563483194, Val Accuracy: 0.997\n",
            "Step 3100: Train Loss=0.00010249971091980115, Val Loss: 0.0018574097011005506, Val Accuracy: 0.998\n",
            "Step 3200: Train Loss=6.104412022978067e-05, Val Loss: 0.0019883662208449097, Val Accuracy: 0.998\n",
            "Step 3300: Train Loss=6.895266415085644e-05, Val Loss: 0.0019896785842720417, Val Accuracy: 0.998\n"
          ]
        }
      ],
      "source": [
        "run_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNEOPRMsGKR"
      },
      "source": [
        "# Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UjRY9u_UsFNm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_all():\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "class TransformerUnitTest:\n",
        "    def __init__(self, gt_vars: dict, verbose=False):\n",
        "        self.gt_vars = gt_vars\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def test_all(self):\n",
        "        self.test_tokenizer()\n",
        "        self.test_ape()\n",
        "        self.test_mha()\n",
        "        self.test_transformer_layer()\n",
        "        self.test_transformer_model()\n",
        "        self.test_scheduler()\n",
        "        self.test_loss()\n",
        "\n",
        "    def test_tokenizer(self):\n",
        "        seed_all()\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('ccpeen', add_cls_token=True),\n",
        "            self.gt_vars['tokenizer_1'],\n",
        "            \"Tokenization with cls class\"\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('cpppencpen', add_cls_token=False),\n",
        "            self.gt_vars['tokenizer_2'],\n",
        "            \"Tokenization without cls class\"\n",
        "        )\n",
        "\n",
        "    def test_ape(self):\n",
        "        seed_all()\n",
        "        ape_result = AbsolutePositionalEncoding(128)(torch.randn((8, 12, 128)))\n",
        "        self.check_correctness(ape_result, self.gt_vars['ape'], \"APE\")\n",
        "\n",
        "    def test_mha(self):\n",
        "        seed_all()\n",
        "        mha_result = MultiHeadAttention(d_model=128, n_heads=4, rpe=False)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result,\n",
        "            self.gt_vars['mha_no_rpe'],\n",
        "            \"Multi-head Attention without RPE\"\n",
        "        )\n",
        "        mha_result_rpe = MultiHeadAttention(d_model=128, n_heads=8, rpe=True)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result_rpe,\n",
        "            self.gt_vars['mha_with_rpe'],\n",
        "            \"Multi-head Attention with RPE\"\n",
        "        )\n",
        "    \n",
        "    def test_transformer_layer(self):\n",
        "        seed_all()\n",
        "        for prenorm in [True, False]:\n",
        "            transformer_layer_result = TransformerLayer(\n",
        "                d_model=128, n_heads=4, prenorm=prenorm, rpe=False\n",
        "            )(torch.randn((8, 12, 128)))\n",
        "            self.check_correctness(\n",
        "                transformer_layer_result,\n",
        "                self.gt_vars[f'transformer_layer_prenorm_{prenorm}'],\n",
        "                f\"Transformer Layer Prenorm {prenorm}\"\n",
        "            )\n",
        "\n",
        "    def test_transformer_model(self):\n",
        "        seed_all()\n",
        "        transformer_model_result = TransformerModel(\n",
        "            ModelConfig(d_model=128, prenorm=True, pos_enc_type='ape') \n",
        "        )(torch.randn((8, 12, 5)))\n",
        "        self.check_correctness(\n",
        "            transformer_model_result,\n",
        "            self.gt_vars['transformer_model_result'],\n",
        "            f\"Transformer Model\"\n",
        "        )\n",
        "\n",
        "    def test_scheduler(self):\n",
        "        model = TransformerModel(ModelConfig()) #original code\n",
        "        #model = nn.Transformer(ModelConfig())\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = CustomScheduler(optimizer, 10_000)\n",
        "        optimizer.step()\n",
        "        scheduler.step(521)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_1'],\n",
        "            f\"Scheduler Warmup\"\n",
        "        )\n",
        "        scheduler.step(2503)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_2'],\n",
        "            f\"Scheduler Cooldown\"\n",
        "        )\n",
        "\n",
        "    def test_loss(self):\n",
        "        seed_all()\n",
        "        model = TransformerModel(ModelConfig()) # original code\n",
        "        # model = nn.Transformer(ModelConfig())\n",
        "        trainer = Trainer(model, TrainerConfig(device='cpu'))\n",
        "        loss_result, _ = trainer.compute_batch_loss_acc(\n",
        "            torch.randn((8, 12, 5)),\n",
        "            torch.ones(8).float(),\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            loss_result,\n",
        "            self.gt_vars['loss'],\n",
        "            f\"Batch Loss\"\n",
        "        )\n",
        "\n",
        "    def check_correctness(self, out, gt, title):\n",
        "        try:\n",
        "            diff = (out - gt).norm()\n",
        "            # print(\"out\", out)\n",
        "            # print(\"gt\",gt)\n",
        "            #print(diff)\n",
        "            # print(out.shape)\n",
        "            # print(gt.shape)\n",
        "            # print(out)\n",
        "            # print(gt)\n",
        "            # print(diff)\n",
        "            #print(diff)\n",
        "        except:\n",
        "            diff = float('inf')\n",
        "        if diff < 1e-4: # increase the epsilon from 1e-5 to 1e-4\n",
        "            print(f\"[Correct] {title}\")\n",
        "        else:\n",
        "            print(f\"[Wrong] {title}\")\n",
        "            if self.verbose:\n",
        "                print(\"-----\")\n",
        "                print(\"Expected: \")\n",
        "                print(gt)\n",
        "                print(\"Received: \")\n",
        "                print(out)\n",
        "                print(\"-----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u2DlMVJ4wMrp"
      },
      "outputs": [],
      "source": [
        "!gdown 1-2-__6AALEfqhfew3sJ2QiCE1-rrFMnQ -q -O unit_tests.pkl\n",
        "import pickle\n",
        "with open('unit_tests.pkl', 'rb') as f:\n",
        "    gt_vars = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D2ultFy8DTMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0399c7-b05f-4546-a035-2bf5277df361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Correct] Tokenization with cls class\n",
            "[Correct] Tokenization without cls class\n",
            "[Correct] APE\n",
            "[Correct] Multi-head Attention without RPE\n",
            "[Correct] Multi-head Attention with RPE\n",
            "[Correct] Transformer Layer Prenorm True\n",
            "[Correct] Transformer Layer Prenorm False\n",
            "[Correct] Transformer Model\n",
            "[Correct] Scheduler Warmup\n",
            "[Correct] Scheduler Cooldown\n",
            "[Correct] Batch Loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "TransformerUnitTest(gt_vars, verbose=False).test_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vti9TaCjSL-e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}